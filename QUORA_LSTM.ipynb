{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f89fb1a64f63d3cb1845778718e695d2e5e3c2f"
   },
   "source": [
    "#  5. Quora Insincere Questions Classification \n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "445677aec882614fdcf5e33c8c3bce80462a1122"
   },
   "source": [
    "###  5.1Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "\n",
    "For LSTM: https://www.kaggle.com/sdelecourt/simple-lstm-that-does-the-job\n",
    "\n",
    "For loading embeddings: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0f031d4e082fa5e36862b1a8851ad739d231d0ae"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "baee81e14b07064398fa8a56fb247bdd92d04384"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "69bae3a9e1017d95eab6ebdd928522054ef2806d"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "47de3b76162ddc7a1785c4a925f208328c4e8915"
   },
   "outputs": [],
   "source": [
    "# Parameters and definitions\n",
    "RANDOM_SEED = 0\n",
    "VAL_SET_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "96e0dae0b6f3d3eb777e0fb4346da37ce9a4c959"
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbb4282c349d40ef1dd3e22eceaefb5028e44356"
   },
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c73b0d56a143137e5cdb3e4f0f7901ffc0987fb4"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/\"\n",
    "TRAIN_SAMPLES = DATA_DIR+\"train.csv\"\n",
    "TEST_SAMPLES = DATA_DIR+\"test.csv\"\n",
    "EMBD_SAMPLES = DATA_DIR+\"embeddings.zip\"\n",
    "SUBMISSION_FILE = DATA_DIR+\"submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9d7185e304b9c0a19afc132b26080ae3d64009b9"
   },
   "outputs": [],
   "source": [
    "# Split training set into training and validation sets\n",
    "df_train, df_val = train_test_split(df_train, test_size=VAL_SET_SIZE, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37cd91a6d394c956fe89c9af479bf8f106545995"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac037022b123f77dea713ac3162ece180bbf9458"
   },
   "source": [
    "### 5.3 Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "905161223c9f653cfa46f0e95501a7b723d0b96b"
   },
   "outputs": [],
   "source": [
    "# File path of pretrained word embeddings\n",
    "EMB_FILE_PATH = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "64756a86ec97cf30419cc7e7e30e9c7b538846eb"
   },
   "outputs": [],
   "source": [
    "# Load GloVe Word Embeddings\n",
    "#Loads word embeddings and returns embeddings index\n",
    "def load_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    f = open(file_path)\n",
    "    for line in tqdm(f):\n",
    "        values = line.split(\" \")\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "03076a8d394b81ca851fbe12da382098d036d268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:26, 10652.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_index = load_embeddings(EMB_FILE_PATH)\n",
    "print('Found %s word vectors.' % len(emb_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "b9c2dbc58142b680e7d24c6425de9166fddd6299"
   },
   "outputs": [],
   "source": [
    "# Extract text and targets from training set\n",
    "train_questions = df_train['question_text'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "# Extract text and targets from validation set\n",
    "val_questions = df_val['question_text'].values\n",
    "y_val = df_val['target'].values\n",
    "\n",
    "# Extract text and targets from test set\n",
    "test_questions = df_test['question_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "1e7144dce026d6e0a1555eec7a9a76bdc9cf2e07"
   },
   "outputs": [],
   "source": [
    "# Number of unique words in our dataset\n",
    "NUM_UNIQUE_WORDS = 1044897\n",
    "# Maximum number of words in a question\n",
    "MAX_WORDS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7ae149f244cd527d9731b0e57a75d8d58372c6e9"
   },
   "outputs": [],
   "source": [
    "#Returns tokenizer\n",
    "def get_tokenizer(num_unique_words):\n",
    "    return Tokenizer(num_words=num_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "98c2fc6103600fe13da54a6e82255bf37743fbdc"
   },
   "outputs": [],
   "source": [
    "# Convert questions into vectors of integers using Keras Tokenizer\n",
    "tokenizer = get_tokenizer(NUM_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(list(train_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "c01976e744017f5eb19ba8c2283e842f9c0caab4"
   },
   "outputs": [],
   "source": [
    "# Store tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('LSTM_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "d9244ae8f062270738c964fcaf6d8dd3a6f79fcf"
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_questions)\n",
    "X_val = tokenizer.texts_to_sequences(val_questions)\n",
    "X_test = tokenizer.texts_to_sequences(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "859b4e3ae1d326d9bfe0bab0568ace588c73ae47"
   },
   "outputs": [],
   "source": [
    "# Pad sequences so that they are all the same length. Questions shorter than maxlen are padded with zeros.\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_WORDS)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=MAX_WORDS)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "e69e2061384e16181b44afc35a209feff204cd1a"
   },
   "outputs": [],
   "source": [
    "# Create word index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "93756cfad8bdaf7e0d81a393a846591611ed756b"
   },
   "outputs": [],
   "source": [
    "# Dimension of embedding matrix\n",
    "EMB_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f64eec00d404e6dd4292d6e26c85d22efe734348"
   },
   "source": [
    "### 5.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "226d8820480cef1681926dec74a1cdc143c64971"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMB_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = emb_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "5ec10ee2ed7f8a3cb10ea9dffeae1ff51f6f03d5"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMB_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_WORDS,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "8287ecb22ee41fadab53b82147c9490552490903"
   },
   "outputs": [],
   "source": [
    "# dimensionality of output space\n",
    "lstm_out = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "5f8123d3eeefadb26dd53c3c21078e105dbfba65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 125, 300)          47323500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 47,724,501\n",
      "Trainable params: 401,001\n",
      "Non-trainable params: 47,323,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_out = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "b57fc31a5689a852eee9539819c7a1b51cc52ea9"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "#Creates LSTM model with embedding layer, LSTM and dense layer\n",
    "def create_LSTM(embedding_layer):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "f9c0caa75391a6fade8dad557b105c16585ffe81"
   },
   "outputs": [],
   "source": [
    "LSTM_model = create_LSTM(emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "2e9466695454a7a7b2bc5ae445c3fac54a45bf85",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/2\n",
      "1044897/1044897 [==============================] - 592s 566us/step - loss: 0.1394 - acc: 0.9469 - val_loss: 0.1227 - val_acc: 0.9528\n",
      "Epoch 2/2\n",
      "1044897/1044897 [==============================] - 590s 564us/step - loss: 0.1213 - acc: 0.9530 - val_loss: 0.1166 - val_acc: 0.9547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2f4314fd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model to training data\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=2, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "85aefa0b189941c7a1fdc4036f0e6ce2d7849a7a"
   },
   "outputs": [],
   "source": [
    "# Save model \n",
    "model.save('LSTM_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "1ad70125af3bd5c987a48a3a24fb0e75035d082d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261225/261225 [==============================] - 932s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set \n",
    "y_pred_val = model.predict(X_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "9dd7e88d3ba54f48b9d87cdc3732b37ae3d030d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 339936/1044897 [========>.....................] - ETA: 42:30"
     ]
    }
   ],
   "source": [
    "# Make predictions for training set\n",
    "y_pred_train = model.predict(X_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "6b308c4b5d77cbf405eca8c1fff97c32c3fb6652"
   },
   "outputs": [],
   "source": [
    "# Convert probabilities into predictions for validation set\n",
    "y_te_val = (np.array(y_pred_val) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "6cd185348f688d47c599ef501c34c55f57c73116"
   },
   "outputs": [],
   "source": [
    "# Convert probabilities into predictions for training set\n",
    "y_te_train = (np.array(y_pred_train) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19e0fb0267c88a5a03b9635ccb39e00f42ac05cb"
   },
   "source": [
    "###  5.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "84e4fdaaf120173df4fe069bc9f4faace04a0ee7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "68c057a8c27aeccaa636be717f4bf55fc80198e9"
   },
   "outputs": [],
   "source": [
    "#Produces a report containing the accuracy, f1-score, precision and recall metrics.\n",
    "def produce_metrics(y, y_pred):\n",
    "\n",
    "    print(\"Accuracy: {}, F1 Score: {}, Precision: {}, Recall: {}\".format(accuracy_score(y, y_pred),\n",
    "                                                                     f1_score(y, y_pred, average=\"macro\"),\n",
    "                                                                     precision_score(y, y_pred, average=\"macro\"),\n",
    "                                                                     recall_score(y, y_pred, average=\"macro\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "404c38412719c03eabcecdb893820a1f890f9ba3"
   },
   "outputs": [],
   "source": [
    "#Produces a classification report.\n",
    "def produce_classification_report(y, y_pred):\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "98594181a30d12da6b1bcd4635e0dbd82fdc8a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954717197817973, F1 Score: 0.777098944834635, Precision: 0.8227636210799454, Recall: 0.7442151677611695\n"
     ]
    }
   ],
   "source": [
    "produce_metrics(y_val, y_te_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "d1a864776e972fe3bef5e8bf762f59a60c9e7b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555592560797859, F1 Score: 0.7835296667591048, Precision: 0.82829239606895, Recall: 0.7509097990272653\n"
     ]
    }
   ],
   "source": [
    "produce_metrics(y_train, y_te_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "104ff56accf0f57649193732969c66180a99d12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98    245149\n",
      "           1       0.68      0.50      0.58     16076\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    261225\n",
      "   macro avg       0.82      0.74      0.78    261225\n",
      "weighted avg       0.95      0.95      0.95    261225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_classification_report(y_val, y_te_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "f9939c907ade700a96cc2cea004269435a204642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98    980163\n",
      "           1       0.69      0.52      0.59     64734\n",
      "\n",
      "   micro avg       0.96      0.96      0.96   1044897\n",
      "   macro avg       0.83      0.75      0.78   1044897\n",
      "weighted avg       0.95      0.96      0.95   1044897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_classification_report(y_train, y_te_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "13593b9699f930ff590388fada288805c7cbf3ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954717197817973, F1 Score: 0.777098944834635\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}, F1 Score: {}\".format(accuracy_score(y_val, y_te_val), \n",
    "                                          f1_score(y_val, y_te_val, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook We built LSTM model.We will explore MLP in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
